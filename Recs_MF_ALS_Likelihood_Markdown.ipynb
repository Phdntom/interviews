{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "In a purely mathematical sense, given a matrix $R$, the objective of `matrix factorization` is to find two `factor matrices` to reconstruct / approximate $R$ as $\\hat{R} = P^T Q$ where $R$ is $m \\times n$, $P$ is $m \\times f$, $Q$ is $n \\times f$ and $f$ is the hyperparameter which chooses the dimensionality of the approximation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood\n",
    "\n",
    "While $R$, $P$ and $Q$ are the standard letter names for the mathematical problem, in statistics the names are less standard. I will now start using a notation that is more useful/common but is nonetheless highly variable within the machine learning community, and note that $R$ is usually a `rating matrix`, viz.\n",
    "\n",
    "$$R = X^T Y$$\n",
    "\n",
    "Model the observed matrix $R$ as being drawn from normally distributed ratings as\n",
    "\n",
    "$$R_{u,i} \\sim \\mathcal{N}(X_u^TY_i, \\lambda^{-1}_R)$$\n",
    "\n",
    "where the normal distribution has mean $\\mu=X_u^TY_i$ and variance $\\sigma^2 = \\lambda^{-1}_r$.\n",
    "\n",
    "and factor matrices are also normally distributed with mean zero and variance $\\lambda^{-1}_X$ and $\\lambda^{-1}_Y$ along $f$ dimensions,\n",
    "\n",
    "$$X_u \\sim \\mathcal{N}(0, \\lambda_X^{-1}I_f)$$\n",
    "\n",
    "$$Y_i \\sim \\mathcal{N}(0, \\lambda_Y^{-1}I_f)$$\n",
    "\n",
    "This would give a likelihood $\\mathcal{L}$ as\n",
    "\n",
    "$$\\mathcal{L} = \\prod_{u,i} \\mathcal{N}(X_u^TY_i, \\lambda^{-1}_r) \\mathcal{N}(0, \\lambda_X^{-1}I_f)   \\mathcal{N}(0, \\lambda_Y^{-1}I_f). $$\n",
    "\n",
    "Since it is easier to work with a sum than a product and the log is convex, we can form the log-likelihood as\n",
    "\n",
    "$$logL = \\frac{\\lambda_p}{2}\\sum_{u,i} (r_{ui} - X_u^TY_i)^2 + \\frac{\\lambda_X}{2}\\sum_u \\|X_u\\|^2 + \\frac{\\lambda_Y}{2}\\sum_i \\|Y_i\\|^2$$\n",
    "\n",
    "and its extremum values will be the same as the likelihood. Depending on whether you are from the stats camp and want to maximize the $logL$ or from the ML camp and want to minimize $-logL$, either way you have to take the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit Data and Implicit Matrix Factorization\n",
    "\n",
    "Historically speaking, early problems in recommender systems tried to predict a `rating` a user $u$ would give to item $i$.\n",
    "\n",
    "More recently, that approach has fallen out of favor with an emphasis shifting to `preference` prediction; concretely we now aim to predict the preference and/or future consumption of user $u$ for/of item $i$.\n",
    "\n",
    "This shift also highlights the move away from `explicit data` toward `implicit data` because of a few key advantages --\n",
    "\n",
    "* Implicit data is usually more plentiful and is already naturally collected by many services, e.g. at Staples we already know what people purchase, regardless of if they tell us a rating of that product.\n",
    "\n",
    "* Ratings are more noisy because people may change how they rate items based on mood, time of day, personal bias to rate higher or lower than other users, etc.\n",
    "\n",
    "* Predicting a preference might more closely match the end goal of a system/product since even if you correctly identify how a user may rate an item, there is no guarantee the user will consume that item in the first place.\n",
    "\n",
    "In practice we observe a matrix $C$, often called the `click matrix` or also the `consumption matrix`.\n",
    "\n",
    "The goal of `implicit matrix factorization` is to reconstruct the binary preference matrix $P$.\n",
    "\n",
    "Based on the observed values (non-zero) and unobserved values (zero) of $C$, we create the binary preference matrix $P$. \n",
    "\n",
    "$$p_{ui} = \\begin{cases}\n",
    "0 &\\quad c_{ui} = 0\\\\\n",
    "1 &\\quad c_{ui} > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We use $P$ as the ground truth in training and evaluation throughout.\n",
    "\n",
    "# Weighted Matrix Factorization (WMF)\n",
    "\n",
    "Weighted Matrix factorization differs from the formulation outlined at the top by adding a weight value $w_{ui}$ associated with each preference prediction $p_{ui}$. This addition is unorthodox in the maximum likelihood formulation and it's not possible to construct the likelihood as we did above in the same way. From here, we will treat this weighting factor as a simple added heuristic and express the weighted matrix factorization cost function --\n",
    "\n",
    "$$J = \\sum_{u,i} w_{ui}\\big( p_{ui} - X_u^TY_i\\big)^2 + \\lambda_X \\sum_u \\|X_u\\|^2 + \\lambda_Y\\sum_i \\|Y_i\\|^2$$\n",
    "\n",
    "To minimize the cost function we take the derivative with respect to the two factor matrices\n",
    "\n",
    "$$0 = \\frac{\\partial J}{\\partial X_v} = \\sum_{u,i} -2 w_{ui} \\big(p_{ui} - X_u^TY_i\\big)Y_i\\frac{\\partial X_u}{\\partial X_v} + 2\\lambda_X \\sum_u X_u \\frac{\\partial X_u}{\\partial X_v}$$\n",
    "\n",
    "$$0 = \\frac{\\partial J}{\\partial Y_j} = \\sum_{u,i} -2 w_{ui} \\big(p_{ui} - X_u^TY_i\\big)X_u\\frac{\\partial Y_i}{\\partial Y_j} + 2\\lambda_Y \\sum_i Y_i \\frac{\\partial Y_i}{\\partial Y_j}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Least Squares Regression (ALS)\n",
    "\n",
    "The set of derivatives $\\frac{\\partial J}{\\partial X_v}$ and $\\frac{\\partial J}{\\partial Y_j}$ above create a non-convex problem, but taking each derivative separately, the problem is convex and has a closed form solution.\n",
    "\n",
    "The alternating least squares (ALS) approximation attempts to find the optimal parameters to the WMF cost function by alternatively performing closed formed updates to the $X_u$ and the $Y_i$, often referred to as user and item updates.\n",
    "\n",
    "Because the equations for the optimal $X_u$ and $Y_i$ are symmetric, we will treat just a single equation below, namely, the equation for $X_u$. We recover the results for $Y_i$ by swapping $X \\leftrightarrow Y$ and index $u \\leftrightarrow i$ in all the following results.\n",
    "\n",
    "Additionally, the sparsity of the observed click matrix $C$ can provide substantial computational speed up if properly exploited during computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Update Equation\n",
    "\n",
    "$$0 = \\sum_{u,i} - w_{ui} \\big(p_{ui} - X_u^TY_i\\big)Y_i\\frac{\\partial X_u}{\\partial X_v} + \\lambda_X \\sum_u X_u \\frac{\\partial X_u}{\\partial X_v}$$\n",
    "\n",
    "$$\\frac{\\partial X_u}{\\partial X_v} = \\delta_{u,v}$$\n",
    "\n",
    "$$0 = -\\sum_i \\big[w_{ui} \\big(p_{ui} - X_u^TY_i\\big)Y_i\\big] + \\lambda_X X_u$$\n",
    "\n",
    "$$\\sum_i w_{ui} X_uY_i^TY_i + \\lambda_X X_u = \\sum_i w_{ui} p_{ui}Y_i$$\n",
    "\n",
    "$$\\Big(\\sum_i w_{ui} Y_i^TY_i + \\lambda_X I \\Big) X_u = \\sum_i w_{ui} p_{ui}Y_i$$\n",
    "\n",
    "We can also right this in matrix form, i.e. rather than find the contribution from each item $i$ in the sum, we can find the contribution from all items for this user simultaneously,\n",
    "\n",
    "$$ \\big(Y^TW^uY + \\lambda I_f \\big) X_u = Y^TW^up_u$$\n",
    "\n",
    "where $W^u$ is a $n \\times n$ square item matrix with the weights along the diagonal and $p_u$ is a n-dimensional vector with the user preference score for each item.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse and Fast Updates\n",
    "\n",
    "Naive computation of the above update would be slow due to the all item matrix products in $Y^TW^uY$.\n",
    "\n",
    "Although in the end it will be optimal to perform vectorized updates with the entire matrix $Y$, for illustrating how we optimize for fast updates it is best to return to the sum on each item $i$.\n",
    "\n",
    "The goal is to only sum over the observed values for each user update and take advantage of the sparsity in the consumption matrix.  Additionally, at some point we want to weight observed and unobserved values differently, and we can proceed here without knowing any specifics around how we will choose those different weights. In that spirit, we will split the sums between the values, $c_{ui} = 0$ and $c_{ui} > 0$ and assigned the following different weights:\n",
    "\n",
    "$$w_{ui} = \\begin{cases}\n",
    "w_0 &\\quad c_{ui} = 0\\\\\n",
    "w_1 &\\quad c_{ui} > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\\Big(\\sum_{i \\in c_{ui} = 0} w_0 Y_i^TY_i + \\sum_{i\\in c_{ui} > 0} w_1 Y_i^TY_i + \\lambda_X I_f \\Big) X_u = \\sum_{i \\in c_{ui} = 0} w_0 p_{ui}Y_i + \\sum_{i \\in c_{ui} > 0} w_1 p_{ui}Y_i$$\n",
    "\n",
    "We work on the left side first and express the sum on unobserved values in terms of the sum on all values minus the sum on observed values.\n",
    "\n",
    "$$ \\sum_{i \\in c_{ui} = 0}w_0 Y_i^TY_i = \\sum_i w_0 Y_i^TY_i - \\sum_{i \\in c_{ui} > 0}w_0 Y_i^TY_i$$\n",
    "\n",
    "On the left side, substituting in and combining the sum on observed values, we arrive at the left sum being independent of the user $u$ and right sum on observed values only, taking advantage of the sparsity of $C$.\n",
    "\n",
    "$$\\Big(\\sum_i w_0 Y_i^TY_i + \\sum_{i\\in c_{ui} > 0} (w_1-w_0) Y_i^TY_i + \\lambda_X I \\Big) X_u = \\sum_{i \\in c_{ui} = 0} w_0 p_{ui}Y_i + \\sum_{i \\in c_{ui} > 0} w_1 p_{ui}Y_i$$\n",
    "\n",
    "For the right side, we note that all terms directly multiply the preference matrix $P$. In the case of the sum on unobserved values, $p_{ui}=0$ by construction, and  the entire sum is zero.\n",
    "\n",
    "$$\\Big(\\sum_i w_0 Y_i^TY_i + \\sum_{i\\in c_{ui} > 0} (w_1-w_0) Y_i^TY_i + \\lambda_X I \\Big) X_u = \\sum_{i \\in c_{ui} > 0} w_1 p_{ui}Y_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Scenario\n",
    "\n",
    "Let's take the following real scenario make one final simplification -\n",
    "\n",
    "$$w_{ui} = 1 + \\alpha c_{ui}$$\n",
    "\n",
    "This choice for $w_{ui}$ corresponds to the above choices for $w_0$ and $w_1$:\n",
    "\n",
    "$$\\Big(\\sum_i Y_i^TY_i + \\sum_{i\\in c_{ui} > 0} \\alpha c_{ui} Y_i^TY_i + \\lambda_X I_f \\Big) X_u = \\sum_{i \\in c_{ui} > 0} (1+\\alpha c_{ui}) p_{ui}Y_i$$\n",
    "\n",
    "Finally, going back to the vectorized matrix update,\n",
    "\n",
    "$$ \\big(Y^TY + \\alpha Y^TC^uY + \\lambda_X I_f \\big)X_u = Y^T (I_n + \\alpha C^u)p_u$$\n",
    "\n",
    "Dimensions check\n",
    "$$(f x n) (n x f) + (f x n)(n x n)(n x f) + (f x f)] (f x 1) = (f x n) (n x n) (n x 1)$$\n",
    "\n",
    "It is useful to define and precompute $S^u = \\alpha C^u$ since they appear together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_surplus_confidence_matrix(C, alpha):\n",
    "    # To construct the surplus confidence matrix, we need to operate only on\n",
    "    # the nonzero elements.\n",
    "    # This is not possible: S = alpha * C\n",
    "    S = C.copy()\n",
    "    S.data = alpha * S.data\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recompute_factors_full(Y, S, lambda_reg):\n",
    "    \"\"\"\n",
    "    Written from the point of view of updating the user factors X with\n",
    "    fixed item vectors Y.\n",
    "    \"\"\"\n",
    "    m = S.shape[0]  # m = number of users\n",
    "    f = Y.shape[1]  # f = number of factors\n",
    "    YTY = np.dot(Y.T, Y)  # precompute this\n",
    "    YTYpR = YTY + lambda_reg * np.eye(f)\n",
    "    A_stack = np.empty((m, f), dtype='float32')\n",
    "    B_stack = np.empty((m, f, f), dtype='float32')\n",
    "\n",
    "    for u in xrange(m):\n",
    "        s_u, idx_u = get_row(S, u)\n",
    "        # i_u indexes the nz item elements for user u\n",
    "        Y_u = Y[idx_u]  # exploit sparsity\n",
    "        A = (s_u + 1).dot(Y_u)\n",
    "        YTSY = np.dot(Y_u.T, (Y_u * s_u[:, None]))\n",
    "        B = YTSY + YTYpR\n",
    "        A_stack[u] = A\n",
    "        B_stack[u] = B\n",
    "    X_stack = solve_sequential(A_stack, B_stack)\n",
    "    \n",
    "    return X_stack\n",
    "\n",
    "def get_row(S, i):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        S: scipy.sparse.csr_matrix\n",
    "        i: index to the ith row in S\n",
    "        \n",
    "    Returns:\n",
    "        S.data[lo:hi]: the non-zero elements in row i of S\n",
    "        S.indices[lo:hi]: the column indices of the non-zero elements in row i of S\n",
    "    \"\"\"\n",
    "    lo, hi = S.indptr[i], S.indptr[i + 1]\n",
    "    return S.data[lo:hi], S.indices[lo:hi]\n",
    "\n",
    "def solve_sequential(As, Bs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X_stack = np.empty_like(As, dtype=As.dtype)\n",
    "    for k in xrange(As.shape[0]):\n",
    "        X_stack[k] = np.linalg.solve(Bs[k], As[k])\n",
    "    return X_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3a443e7e8b1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0minit_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_surplus_confidence_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# summarize WMF_ML20M\n",
    "num_factors = 100\n",
    "num_iters = 5\n",
    "lambda_X_reg = 1e-5\n",
    "lambda_Y_reg = 1e-5\n",
    "alpha = 10\n",
    "init_std=0.01\n",
    "\n",
    "S = linear_surplus_confidence_matrix(train_data, alpha=alpha)\n",
    "\n",
    "num_users, num_items = S.shape\n",
    "\n",
    "ST = S.T.tocsr()\n",
    "\n",
    "random_state=98765\n",
    "if type(random_state) is int:\n",
    "    np.random.seed(random_state)\n",
    "elif random_state is not None:\n",
    "    np.random.setstate(random_state)\n",
    "    \n",
    "# T: transpose\n",
    "# u: user index\n",
    "# i: item index\n",
    "\n",
    "# f: number of latent factors\n",
    "\n",
    "# S: surplus click matrix\n",
    "\n",
    "# X: user factors\n",
    "# Y: item factors\n",
    "\n",
    "Y = np.random.randn(num_items, num_factors).astype('float32') * init_std\n",
    "\n",
    "old_ndcg = -np.inf\n",
    "for i in xrange(num_iters):\n",
    "\n",
    "    X = recompute_factors_full(Y, S, lambda_X_reg)\n",
    "    \n",
    "    Y = recompute_factors_full(X, ST, lambda_Y_reg)\n",
    "\n",
    "    vad_ndcg = rec_eval.normalized_dcg_at_k(S, vad_data, X, Y, k=100, batch_users=5000)\n",
    "\n",
    "    if old_ndcg > vad_ndcg:\n",
    "        break\n",
    "    old_ndcg = vad_ndcg\n",
    "    X_best = X.copy()\n",
    "    Y_best = Y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
